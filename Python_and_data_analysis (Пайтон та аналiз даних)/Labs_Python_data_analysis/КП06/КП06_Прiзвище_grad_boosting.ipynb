{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градієнтний бустінг своїми руками\n",
    "\n",
    "У цьому завданні буде використовуватися датасета `boston` з` sklearn.datasets`. Залиште останні 25% об'єктів для контролю якості, розділивши `X` і` y` на `X_train`,` y_train` і `X_test`,` y_test`.\n",
    "\n",
    "Метою завдання буде реалізувати простий варіант градиентного бустінга над регресійними деревами для випадку квадратичної функції втрат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Встановлення найновіших біблотек, щоб не було проблем з числовими відповідями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перевірочні дані для лабораторної роботи отримані з використанням таких версій бібліотек \n",
    "!pip install \"scikit-learn == 0.24.2\"\n",
    "!pip install \"numpy == 1.22.4\"\n",
    "!pip install \"pandas == 1.5.2\"\n",
    "!pip install \"xgboost == 1.7.1\"\n",
    "# !python --version # Python 3.9.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "from sklearn import ensemble , model_selection,  datasets, metrics, tree, linear_model \n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn 0.24.2\n",
      "numpy 1.22.4\n",
      "pandas 1.5.2\n",
      "xgboost 1.7.1\n"
     ]
    }
   ],
   "source": [
    "print('sklearn',sklearn.__version__)\n",
    "print('numpy',np.__version__)\n",
    "print('pandas',pd.__version__)\n",
    "print('xgboost',xgb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Версії**\n",
    "- sklearn 0.24.2\n",
    "- numpy 1.22.4\n",
    "- pandas 1.5.2\n",
    "- xgboost 1.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання 1\n",
    "\n",
    "Як ви вже знаєте з лекцій, **бустінг** - це метод побудови композицій базових алгоритмів за допомогою послідовного додавання до поточної композиції нового алгоритму з деяким коефіцієнтом.\n",
    "\n",
    "Метод найшвидшого бустінгу навчає кожен новий алгоритм так, щоб він наближав антіградіент помилки за відповідями композиції на навчальній вибірці. Аналогічно мінімізації функцій методом градієнтного спуску, в градієнтному бустінгу ми підправляємо композицію, змінюючи алгоритм в напрямку антіградіента помилки.\n",
    "\n",
    "Скористайтеся формулою з лекцій, яка задає відповіді на навчальній вибірці, на які потрібно навчати новий алгоритм (фактично це лише трохи більш докладно розписаний градієнт від помилки), і отримаєте частковий її випадок, якщо функція втрат `L` - квадрат відхилення відповіді композиції `a(x)`від правильної відповіді `y` на даному `x`.\n",
    "\n",
    "Якщо ви давно не брали похідну самостійно, вам допоможе таблиця похідних елементарних функцій (яку нескладно знайти в інтернеті) і правило диференціювання складної функції. Після диференціювання квадрата у вас виникне множник 2 - тому що нам все одно доведеться вибирати коефіцієнт, з яким буде додано новий базовий алгоритм, проігноіруйте цей множник при подальшому побудові алгоритму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання 2\n",
    "\n",
    "Заведіть масив для об'єктів `DecisionTreeRegressor` (будемо їх використовувати в якості базових алгоритмів) і для дійсних чисел (це будуть коефіцієнти перед базовими алгоритмами).\n",
    "\n",
    "У циклі від навчіть послідовно 50  дерев рішень з параметрами `max_depth = 5` і` random_state = 42` (інші параметри - за замовчуванням). У бустінге часто використовуються сотні і тисячі дерев, але ми обмежимося 50, щоб алгоритм працював швидше, і його було простіше налагоджувати (тому що мета завдання розібратися, як працює метод). Кожне дерево має навчатися на одній і тій же множині  об'єктів, але відповіді, які вчиться прогнозувати дерево, будуть змінюватися у відповідністі з отриманими в завданні 1 правилом.\n",
    "\n",
    "Спробуйте для початку завжди брати коефіцієнт рівним 0.9. Зазвичай виправдано вибирати коефіцієнт значно меншим - близько 0.05 або 0.1, але тому що в нашому навчальному прикладі на стандартному датасета буде всього 50 дерев, візьмемо для початку крок побільше.\n",
    "\n",
    "В процесі реалізації навчання вам буде потрібно функція, яка буде обчислювати прогноз побудованої на даний момент композиції дерев на вибірці `X`:\n",
    "\n",
    "```\n",
    "def gbm_predict (X):\n",
    "    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]\n",
    "(Вважаємо, що base_algorithms_list - список з базовими алгоритмами, coefficients_list - список з коефіцієнтами перед алгоритмами)\n",
    "```\n",
    "\n",
    "Ця ж функція допоможе вам отримати прогноз на контрольній вибірці і оцінити якість роботи вашого алгоритму за допомогою `mean_squared_error` в` sklearn.metrics`.\n",
    "\n",
    "Підведіть результат в ступінь 0.5, щоб отримати `RMSE`. Отримане значення `RMSE` - **відповідь в пункті 2**.\n",
    "\n",
    "Передайте відповідь в функцію **write_answer_2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer_2(*answer):\n",
    "    with open(\"GB_problem2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(round(num,3)) for num in answer]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання 3\n",
    "\n",
    "Вас може також турбувати, що рухаючись з постійним кроком, поблизу мінімуму помилки відповіді на навчальній вибірці змінюються занадто різко, перескакуючи через мінімум.\n",
    "\n",
    "Спробуйте зменшувати вагу перед кожним алгоритмом з кожною наступною ітерацією за формулою `0.9 / (1.0 + i)`, де `i` - номер ітерації (від 0 до 49). Використовуйте якість роботи алгоритму як **відповідь в пункті 3**.\n",
    "\n",
    "Передайте відповідь в функцію **write_answer_3**.\n",
    "\n",
    "У реальності часто застосовується така стратегія вибору кроку: як тільки обраний алгоритм, підберемо коефіцієнт перед ним чисельним методом оптимізації таким чином, щоб відхилення від правильних відповідей було мінімальним. Ми не будемо пропонувати вам реалізувати це для виконання завдання, але рекомендуємо спробувати розібратися з такою стратегією і реалізувати її при нагоді для себе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer_3(*answer):\n",
    "    with open(\"GB_problem3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(round(num,3)) for num in answer]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання 4\n",
    "\n",
    "Реалізований вами метод - градієнтний бустінг над деревами - дуже популярний в машинному навчанні. Він представлений як в самій бібліотеці `sklearn`, так і в сторонньої бібліотеці `XGBoost`, яка має свій пітоновскій інтерфейс. На практиці `XGBoost` працює помітно краще` GradientBoostingRegressor` з `sklearn`, але для цього завдання ви можете використовувати будь-яку реалізацію (можете використовувати або `XGBoost`, або `GradientBoostingRegressor`).\n",
    "\n",
    "Досліджуйте, перенавчається чи градієнтний бустінг з ростом числа ітерацій (і подумайте, чому), а також із зростанням глибини дерев. На основі спостережень випишіть через пробіл номери правильних з наведених нижче тверджень в порядку зростання номера (це буде **відповідь в п.4**):\n",
    "\n",
    "    1. Зі збільшенням числа дерев, починаючи з деякого моменту, якість роботи градиентного бустінга не змінюється істотно.\n",
    "\n",
    "    2. Зі збільшенням числа дерев, починаючи з деякого моменту, градієнтний бустінг починає перенавчатися.\n",
    "\n",
    "    3. З ростом глибини дерев, починаючи з деякого моменту, якість роботи градиентного бустінга на тестовій вибірці починає погіршуватися.\n",
    "\n",
    "    4. З ростом глибини дерев, починаючи з деякого моменту, якість роботи градиентного бустінга перестає суттєво змінюватися\n",
    "    \n",
    "Передайте відповідь в функцію **write_answer_4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ввести необхідну кількість аргументів чрез кому в зростаючому порядку\n",
    "def write_answer_4(*answer): \n",
    "    with open(\"GB_problem4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(num) for num in answer]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання 5\n",
    "\n",
    "Порівняйте отриману за допомогою градієнтного бустінга якість з якістю роботи лінійної регресії.\n",
    "\n",
    "Для цього навчіть `LinearRegression` з` sklearn.linear_model` (з типовими параметрами) на навчальній вибірці та оцініть для прогнозів отриманого алгоритму на тестовій вибірці `RMSE`. Отримане якість - відповідь в **пункті 5**.\n",
    "\n",
    "Передайте відповідь в функцію **write_answer_5**.\n",
    "\n",
    "В даному прикладі якість роботи простої моделі повинно було виявитися гірше, але не варто забувати, що так буває не завжди. У завданнях ви можете ще зустріти приклад зворотній ситуації."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer_5(*answer):\n",
    "    with open(\"GB_problem5.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(round(num,3)) for num in answer]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
